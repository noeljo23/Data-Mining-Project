{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from googleapiclient.discovery import build\n",
    "\n",
    "# -----------------------------\n",
    "# Global Setup: API Key and YouTube Client\n",
    "# -----------------------------\n",
    "API_KEY = '<API>'  # Replace with your actual API key\n",
    "youtube = build('youtube', 'v3', developerKey=API_KEY)\n",
    "\n",
    "# ============================================================\n",
    "# Part 1: Fetch Videos via Date-Segmented Search Query (\"Artificial Intelligence\")\n",
    "# ============================================================\n",
    "def fetch_searched_videos():\n",
    "    query = \"Artificial Intelligence\"\n",
    "    max_results_per_page = 100\n",
    "\n",
    "    # Helper function to generate date ranges\n",
    "    def generate_date_ranges(start_date, end_date, delta_months=1):\n",
    "        date_ranges = []\n",
    "        current_start = start_date\n",
    "        while current_start < end_date:\n",
    "            current_end = current_start + relativedelta(months=delta_months)\n",
    "            if current_end > end_date:\n",
    "                current_end = end_date\n",
    "            date_ranges.append({\n",
    "                'publishedAfter': current_start.strftime(\"%Y-%m-%dT%H:%M:%SZ\"),\n",
    "                'publishedBefore': current_end.strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "            })\n",
    "            current_start = current_end\n",
    "        return date_ranges\n",
    "\n",
    "    # Define the overall date range and segmentation period\n",
    "    start_date = datetime.datetime(2024, 1, 1)\n",
    "    end_date = datetime.datetime(2025, 1, 1)\n",
    "    date_ranges = generate_date_ranges(start_date, end_date, delta_months=1)\n",
    "\n",
    "    # Gather all video IDs across all segments\n",
    "    all_video_ids = []\n",
    "    for segment in date_ranges:\n",
    "        next_page_token = None\n",
    "        print(f\"Fetching videos from {segment['publishedAfter']} to {segment['publishedBefore']}\")\n",
    "        while True:\n",
    "            request = youtube.search().list(\n",
    "                part=\"snippet\",\n",
    "                q=query,\n",
    "                type=\"video\",\n",
    "                maxResults=max_results_per_page,\n",
    "                pageToken=next_page_token,\n",
    "                publishedAfter=segment['publishedAfter'],\n",
    "                publishedBefore=segment['publishedBefore']\n",
    "            )\n",
    "            response = request.execute()\n",
    "            for item in response.get(\"items\", []):\n",
    "                video_id = item['id'].get('videoId')\n",
    "                if video_id:\n",
    "                    all_video_ids.append(video_id)\n",
    "            next_page_token = response.get(\"nextPageToken\")\n",
    "            if not next_page_token:\n",
    "                break\n",
    "\n",
    "    # Remove duplicate video IDs\n",
    "    all_video_ids = list(set(all_video_ids))\n",
    "    print(f\"Total unique video IDs collected from search: {len(all_video_ids)}\")\n",
    "\n",
    "    # Fetch detailed video information (including snippet, contentDetails, and statistics)\n",
    "    video_details = []\n",
    "    for i in range(0, len(all_video_ids), 50):\n",
    "        ids_chunk = all_video_ids[i:i+50]\n",
    "        request = youtube.videos().list(\n",
    "            part=\"snippet,contentDetails,statistics\",\n",
    "            id=\",\".join(ids_chunk)\n",
    "        )\n",
    "        response = request.execute()\n",
    "        video_details.extend(response.get(\"items\", []))\n",
    "\n",
    "    print(f\"Total videos fetched from search: {len(video_details)}\")\n",
    "\n",
    "    # --- New Section: Fetch Channel Subscriber Counts ---\n",
    "    # Extract unique channel IDs from video details\n",
    "    unique_channel_ids = set()\n",
    "    for video in video_details:\n",
    "        if 'snippet' in video and 'channelId' in video['snippet']:\n",
    "            unique_channel_ids.add(video['snippet']['channelId'])\n",
    "\n",
    "    print(f\"Total unique channel IDs found: {len(unique_channel_ids)}\")\n",
    "\n",
    "    # Create a dictionary to hold the subscriber count for each channel\n",
    "    channel_subscribers = {}\n",
    "    unique_channel_ids = list(unique_channel_ids)\n",
    "\n",
    "    # Process channel IDs in batches of up to 50.\n",
    "    for i in range(0, len(unique_channel_ids), 50):\n",
    "        ids_chunk = unique_channel_ids[i:i+50]\n",
    "        request = youtube.channels().list(\n",
    "            part=\"statistics\",\n",
    "            id=\",\".join(ids_chunk)\n",
    "        )\n",
    "        response = request.execute()\n",
    "        for channel in response.get(\"items\", []):\n",
    "            channel_id = channel.get(\"id\")\n",
    "            stats = channel.get(\"statistics\", {})\n",
    "            # subscriberCount is returned as a string. It may be hidden or missing.\n",
    "            channel_subscribers[channel_id] = stats.get(\"subscriberCount\", None)\n",
    "\n",
    "    # Append subscriber count to each video detail based on its channelId.\n",
    "    for video in video_details:\n",
    "        channel_id = video.get(\"snippet\", {}).get(\"channelId\")\n",
    "        # If the channel id is not found, assign None\n",
    "        video[\"channelSubscriberCount\"] = channel_subscribers.get(channel_id, None)\n",
    "\n",
    "    # Normalize and save the raw search results to CSV\n",
    "    df_search = pd.json_normalize(video_details)\n",
    "    search_csv = 'youtube_search_raw.csv'\n",
    "    df_search.to_csv(search_csv, index=False)\n",
    "    print(f\"Search videos data has been saved to '{search_csv}'.\")\n",
    "\n",
    "# ============================================================\n",
    "# Main Execution\n",
    "# ============================================================\n",
    "if __name__ == \"__main__\":\n",
    "    fetch_searched_videos()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "def iso_duration_to_seconds(duration):\n",
    "    \"\"\"\n",
    "    Converts an ISO8601 duration string (e.g., \"PT5M12S\") to total seconds.\n",
    "    \"\"\"\n",
    "    if pd.isna(duration):\n",
    "        return None\n",
    "    pattern = re.compile(r'PT(?:(\\d+)H)?(?:(\\d+)M)?(?:(\\d+)S)?')\n",
    "    match = pattern.match(duration)\n",
    "    if match:\n",
    "        hours = int(match.group(1)) if match.group(1) else 0\n",
    "        minutes = int(match.group(2)) if match.group(2) else 0\n",
    "        seconds = int(match.group(3)) if match.group(3) else 0\n",
    "        return hours * 3600 + minutes * 60 + seconds\n",
    "    return None\n",
    "\n",
    "def remove_non_ascii(text):\n",
    "    \"\"\"\n",
    "    Removes non-ASCII symbols from text.\n",
    "    \"\"\"\n",
    "    if pd.isnull(text):\n",
    "        return text\n",
    "    return re.sub(r'[^\\x00-\\x7F]+', '', text)\n",
    "\n",
    "def clean_youtube_data(input_csv: str, output_csv: str):\n",
    "    \"\"\"\n",
    "    Loads, cleans, and transforms YouTube video data according to the\n",
    "    provided cleaning steps. Splits publishTime into publishDate & publishTimeUTC.\n",
    "    Removes rows with viewCount < 1000.\n",
    "    \"\"\"\n",
    "    # -----------------------------\n",
    "    # Step 1: Load the CSV into a DataFrame\n",
    "    # -----------------------------\n",
    "    df = pd.read_csv(input_csv)\n",
    "    print(\"Initial columns:\")\n",
    "    print(df.columns.tolist())\n",
    "    print(\"\\nFirst few rows of data:\")\n",
    "    print(df.head())\n",
    "\n",
    "    # -----------------------------\n",
    "    # Step 2: Remove Unwanted Columns\n",
    "    # -----------------------------\n",
    "    columns_to_drop = [\n",
    "        'kind',\n",
    "        'etag',\n",
    "        'snippet.channelId',\n",
    "        'snippet.thumbnails.default.url',\n",
    "        'snippet.thumbnails.default.width',\n",
    "        'snippet.thumbnails.default.height',\n",
    "        'snippet.thumbnails.medium.url',\n",
    "        'snippet.thumbnails.medium.width',\n",
    "        'snippet.thumbnails.medium.height',\n",
    "        'snippet.thumbnails.high.url',\n",
    "        'snippet.thumbnails.high.width',\n",
    "        'snippet.thumbnails.high.height',\n",
    "        'snippet.thumbnails.standard.url',\n",
    "        'snippet.thumbnails.standard.width',\n",
    "        'snippet.thumbnails.standard.height',\n",
    "        'snippet.thumbnails.maxres.url',\n",
    "        'snippet.thumbnails.maxres.width',\n",
    "        'snippet.thumbnails.maxres.height',\n",
    "        'snippet.liveBroadcastContent',\n",
    "        'snippet.localized.title',\n",
    "        'snippet.localized.description',\n",
    "        'contentDetails.dimension',\n",
    "        'contentDetails.definition',\n",
    "        'contentDetails.regionRestriction.allowed',\n",
    "        'contentDetails.projection',\n",
    "        'contentDetails.regionRestriction.blocked'\n",
    "    ]\n",
    "    cols_to_drop = [col for col in columns_to_drop if col in df.columns]\n",
    "    df.drop(columns=cols_to_drop, inplace=True, errors='ignore')\n",
    "\n",
    "    # -----------------------------\n",
    "    # Step 3: Remove Duplicate Rows\n",
    "    # -----------------------------\n",
    "    df.drop_duplicates(inplace=True)\n",
    "\n",
    "    # -----------------------------\n",
    "    # Step 4: Convert Date/Time Values\n",
    "    # -----------------------------\n",
    "    if 'snippet.publishedAt' in df.columns:\n",
    "        df['snippet.publishedAt'] = pd.to_datetime(df['snippet.publishedAt'], errors='coerce')\n",
    "\n",
    "    # -----------------------------\n",
    "    # Step 5: Convert Engagement Statistics to Numeric Types\n",
    "    # -----------------------------\n",
    "    stats_columns = [\n",
    "        'statistics.viewCount',\n",
    "        'statistics.likeCount',\n",
    "        'statistics.favoriteCount',\n",
    "        'statistics.commentCount'\n",
    "    ]\n",
    "    for col in stats_columns:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "    # -----------------------------\n",
    "    # Step 6: Convert Video Duration from ISO 8601 to Seconds\n",
    "    # -----------------------------\n",
    "    if 'contentDetails.duration' in df.columns:\n",
    "        df['duration_seconds'] = df['contentDetails.duration'].apply(iso_duration_to_seconds)\n",
    "\n",
    "    # -----------------------------\n",
    "    # Step 7: Transform List Columns (e.g., snippet.tags)\n",
    "    # -----------------------------\n",
    "    if 'snippet.tags' in df.columns:\n",
    "        df['snippet.tags'] = df['snippet.tags'].apply(lambda x: tuple(x) if isinstance(x, list) else x)\n",
    "\n",
    "    # -----------------------------\n",
    "    # Step 8: Map Category IDs to Names\n",
    "    # -----------------------------\n",
    "    category_mapping = {\n",
    "        1: \"Film & Animation\",\n",
    "        2: \"Autos & Vehicles\",\n",
    "        10: \"Music\",\n",
    "        15: \"Pets & Animals\",\n",
    "        17: \"Sports\",\n",
    "        19: \"Travel & Events\",\n",
    "        20: \"Gaming\",\n",
    "        22: \"People & Blogs\",\n",
    "        23: \"Comedy\",\n",
    "        24: \"Entertainment\",\n",
    "        25: \"News & Politics\",\n",
    "        26: \"Howto & Style\",\n",
    "        27: \"Education\",\n",
    "        28: \"Science & Technology\",\n",
    "        29: \"Nonprofits & Activism\",\n",
    "        30: \"Movies\",\n",
    "        31: \"Shows\"\n",
    "    }\n",
    "    if 'snippet.categoryId' in df.columns:\n",
    "        df['snippet.categoryId'] = pd.to_numeric(df['snippet.categoryId'], errors='coerce')\n",
    "        df['categoryName'] = df['snippet.categoryId'].map(category_mapping)\n",
    "    else:\n",
    "        print(\"Column 'snippet.categoryId' not found. Skipping category mapping.\")\n",
    "\n",
    "    # -----------------------------\n",
    "    # Step 9: Rename Columns for Easier Understanding\n",
    "    # -----------------------------\n",
    "    rename_dict = {\n",
    "        'id': 'videoId',\n",
    "        'snippet.publishedAt': 'publishTime',\n",
    "        'snippet.title': 'title',\n",
    "        'snippet.description': 'description',\n",
    "        'snippet.channelTitle': 'channelTitle',\n",
    "        'snippet.tags': 'tags',\n",
    "        'snippet.categoryId': 'categoryId',\n",
    "        'contentDetails.duration': 'duration',\n",
    "        'contentDetails.caption': 'caption',\n",
    "        'contentDetails.licensedContent': 'licensedContent',\n",
    "        'statistics.viewCount': 'viewCount',\n",
    "        'statistics.likeCount': 'likeCount',\n",
    "        'statistics.favoriteCount': 'favoriteCount',\n",
    "        'statistics.commentCount': 'commentCount',\n",
    "        'snippet.defaultAudioLanguage': 'defaultAudioLanguage',\n",
    "        'snippet.defaultLanguage': 'defaultLanguage',\n",
    "        'duration_seconds': 'durationSeconds'\n",
    "    }\n",
    "    df.rename(columns=rename_dict, inplace=True)\n",
    "\n",
    "    # -----------------------------\n",
    "    # Step 10: Remove Non-ASCII Symbols from Text Columns\n",
    "    # -----------------------------\n",
    "    text_cols = ['title', 'description', 'channelTitle','tags']\n",
    "    for col in text_cols:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].apply(remove_non_ascii)\n",
    "\n",
    "    # -----------------------------\n",
    "    # Step 11: (Optional) Remove Unneeded Columns\n",
    "    # -----------------------------\n",
    "    df.drop(columns=['categoryId', 'duration','favoriteCount'], errors='ignore', inplace=True)\n",
    "\n",
    "    # -----------------------------\n",
    "    # Step 12: Split publishTime into Date & Time (without UTC offset)\n",
    "    # -----------------------------\n",
    "    if 'publishTime' in df.columns:\n",
    "        df['publishDate'] = df['publishTime'].dt.date.astype(str)\n",
    "        df['publishTimeUTC'] = df['publishTime'].dt.strftime('%H:%M:%S')\n",
    "        df.drop(columns=['publishTime'], inplace=True)\n",
    "\n",
    "    # -----------------------------\n",
    "    # Step 13: Filter rows with viewCount < 1000\n",
    "    # -----------------------------\n",
    "    if 'viewCount' in df.columns:\n",
    "        df = df[df['viewCount'] >= 1000]\n",
    "\n",
    "    # -----------------------------\n",
    "    # Step 14: Save the Final Cleaned Data\n",
    "    # -----------------------------\n",
    "    df.to_csv(output_csv, index=False)\n",
    "    print(f\"\\nCleaned data has been saved to '{output_csv}'.\")\n",
    "    print(f\"Total rows in cleaned data: {len(df)}\")\n",
    "    print(\"\\nFinal DataFrame info:\")\n",
    "    print(df.info())\n",
    "    print(\"\\nPreview of cleaned data:\")\n",
    "    print(df.head())\n",
    "\n",
    "# ---------------\n",
    "# Example Usage\n",
    "# ---------------\n",
    "if __name__ == \"__main__\":\n",
    "    input_file = \"/content/youtube_search_raw.csv\"  # Adjust path as needed\n",
    "    output_file = \"cleaned_data.csv\"\n",
    "    clean_youtube_data(input_file, output_file)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
