{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa49550",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching IDs from 2024-01-01T00:00:00Z to 2024-02-01T00:00:00Z\n",
      "Total unique IDs fetched: 227\n",
      "Saved raw data → raw.csv\n",
      "Saved cleaned data → cleaned.csv\n",
      "Rows after cleaning: 162\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import datetime, re, time\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from googleapiclient.discovery import build\n",
    "from googleapiclient.errors import HttpError\n",
    "import os\n",
    "\n",
    "# ─── CONFIG ───────────────────────────────────────────────────────────────────\n",
    "API_KEY    = \"AIzaSyDq6s80go9mLTZ0PmKmxEF8pRKkgpsceh4\"  # ← your API key\n",
    "RAW_CSV    = \"raw.csv\"\n",
    "CLEAN_XLSX = \"cleaned_data.xlsx\"\n",
    "youtube    = build(\"youtube\", \"v3\", developerKey=API_KEY)\n",
    "\n",
    "# ─── HELPER: safe execute with retries ────────────────────────────────────────\n",
    "def safe_execute(request, max_retries=5):\n",
    "    backoff = 1\n",
    "    for _ in range(max_retries):\n",
    "        try:\n",
    "            return request.execute()\n",
    "        except HttpError as e:\n",
    "            status = getattr(e.resp, 'status', None)\n",
    "            if status in (403, 429):\n",
    "                print(f\"→ API rate/quota error {status}, retrying in {backoff}s…\")\n",
    "                time.sleep(backoff)\n",
    "                backoff *= 2\n",
    "                continue\n",
    "            raise\n",
    "    raise RuntimeError(\"Exceeded maximum retries\")\n",
    "\n",
    "# ─── 1) FETCH RAW ─────────────────────────────────────────────────────────────\n",
    "def fetch_raw():\n",
    "    ids = set()\n",
    "    start = datetime.datetime(2024, 1, 1)\n",
    "    end   = datetime.datetime(2025, 1, 1)\n",
    "\n",
    "    while start < end:\n",
    "        after  = start.strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "        before = (start + relativedelta(months=1)).strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "        token = None\n",
    "        print(f\"Fetching IDs from {after} to {before}\")\n",
    "        while True:\n",
    "            req = youtube.search().list(\n",
    "                part=\"id\",\n",
    "                q=\"Artificial Intelligence\",\n",
    "                type=\"video\",\n",
    "                maxResults=50,\n",
    "                publishedAfter=after,\n",
    "                publishedBefore=before,\n",
    "                pageToken=token\n",
    "            )\n",
    "            res = safe_execute(req)\n",
    "            for item in res.get(\"items\", []):\n",
    "                vid = item[\"id\"].get(\"videoId\")\n",
    "                if vid:\n",
    "                    ids.add(vid)\n",
    "            token = res.get(\"nextPageToken\")\n",
    "            if not token:\n",
    "                break\n",
    "            time.sleep(1)\n",
    "        start += relativedelta(months=1)\n",
    "\n",
    "    print(f\"Total unique IDs fetched: {len(ids)}\")\n",
    "\n",
    "    details = []\n",
    "    id_list = list(ids)\n",
    "    for i in range(0, len(id_list), 50):\n",
    "        chunk = id_list[i : i + 50]\n",
    "        req = youtube.videos().list(\n",
    "            part=\"snippet,contentDetails,statistics\",\n",
    "            id=\",\".join(chunk)\n",
    "        )\n",
    "        res = safe_execute(req)\n",
    "        details.extend(res.get(\"items\", []))\n",
    "        time.sleep(1)\n",
    "\n",
    "    df_raw = pd.json_normalize(details)\n",
    "    df_raw.to_csv(RAW_CSV, index=False)\n",
    "    print(f\"Saved raw data → {os.path.abspath(RAW_CSV)}\")\n",
    "\n",
    "# ─── 2) CLEAN & FLATTEN ────────────────────────────────────────────────────────\n",
    "def clean_up():\n",
    "    df = pd.read_csv(RAW_CSV)\n",
    "\n",
    "    # drop thumbnails, etag, kind\n",
    "    drop = [c for c in df.columns if \"thumbnails\" in c or c in (\"etag\", \"kind\")]\n",
    "    df.drop(columns=drop, inplace=True, errors=\"ignore\")\n",
    "\n",
    "    # dedupe\n",
    "    df.drop_duplicates(inplace=True)\n",
    "\n",
    "    # parse publish timestamp\n",
    "    if \"snippet.publishedAt\" in df.columns:\n",
    "        df[\"snippet.publishedAt\"] = pd.to_datetime(df[\"snippet.publishedAt\"], errors=\"coerce\")\n",
    "\n",
    "    # numeric stats\n",
    "    for stat in (\"viewCount\", \"likeCount\", \"commentCount\"):\n",
    "        col = f\"statistics.{stat}\"\n",
    "        if col in df.columns:\n",
    "            df[stat] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "\n",
    "    # convert ISO‐8601 duration to seconds\n",
    "    def iso_to_seconds(d):\n",
    "        m = re.match(r\"PT(?:(\\d+)H)?(?:(\\d+)M)?(?:(\\d+)S)?\", d or \"\")\n",
    "        if not m:\n",
    "            return None\n",
    "        h, mm, s = [int(x or 0) for x in m.groups()]\n",
    "        return h * 3600 + mm * 60 + s\n",
    "\n",
    "    if \"contentDetails.duration\" in df.columns:\n",
    "        df[\"durationSeconds\"] = df[\"contentDetails.duration\"].map(iso_to_seconds)\n",
    "\n",
    "    # rename for clarity\n",
    "    rename_map = {\n",
    "        \"id\": \"videoId\",\n",
    "        \"snippet.publishedAt\": \"publishTime\",\n",
    "        \"snippet.title\": \"title\",\n",
    "        \"snippet.description\": \"description\",\n",
    "        \"snippet.channelTitle\": \"channelTitle\",\n",
    "        \"statistics.viewCount\": \"viewCount\",\n",
    "        \"statistics.likeCount\": \"likeCount\",\n",
    "        \"statistics.commentCount\": \"commentCount\",\n",
    "        \"snippet.tags\": \"tags\",\n",
    "        \"snippet.categoryId\": \"categoryId\",\n",
    "    }\n",
    "    df.rename(columns=rename_map, inplace=True)\n",
    "    df = df.loc[:, ~df.columns.duplicated()]\n",
    "\n",
    "    # split publishTime into date & UTC time\n",
    "    if \"publishTime\" in df.columns:\n",
    "        dt = pd.to_datetime(df[\"publishTime\"], errors=\"coerce\")\n",
    "        df[\"publishDate\"]    = dt.dt.date.astype(str)\n",
    "        df[\"publishTimeUTC\"] = dt.dt.strftime(\"%H:%M:%S\")\n",
    "        df.drop(columns=[\"publishTime\"], inplace=True)\n",
    "\n",
    "    # filter out low‐view videos\n",
    "    if \"viewCount\" in df.columns:\n",
    "        df = df[df[\"viewCount\"] >= 1000]\n",
    "\n",
    "    # remove non-ASCII characters from text columns\n",
    "    text_cols = [\"title\", \"description\", \"channelTitle\", \"tags\"]\n",
    "    for col in text_cols:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].astype(str).apply(lambda x: re.sub(r\"[^\\x00-\\x7F]+\", \"\", x))\n",
    "\n",
    "    # save cleaned to Excel\n",
    "    df.to_excel(CLEAN_XLSX, index=False)\n",
    "    print(f\"Saved cleaned data → {os.path.abspath(CLEAN_XLSX)}\")\n",
    "    print(f\"Rows after cleaning: {len(df)}\")\n",
    "\n",
    "# ─── MAIN ─────────────────────────────────────────────────────────────────────\n",
    "if __name__ == \"__main__\":\n",
    "    fetch_raw()\n",
    "    clean_up()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
